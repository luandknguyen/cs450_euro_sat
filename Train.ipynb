{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from osgeo import gdal\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.optim\n",
    "import torchvision as vision\n",
    "import torchvision.transforms.v2 as v2\n",
    "import tqdm\n",
    "import matplotlib.pyplot as pyplot\n",
    "from typing import Callable, Tuple, List, Any, Union\n",
    "from pathlib import Path\n",
    "\n",
    "import models\n",
    "\n",
    "has_cuda = torch.cuda.is_available()\n",
    "if has_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classifier Model (2-layer CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2000\n",
    "TRAIN_SET_SIZE = 0.8\n",
    "\n",
    "def target_transform(label: int) -> torch.Tensor:\n",
    "    \"\"\"Transform label into one-hot encoded.\"\"\"\n",
    "    l = torch.zeros(10)\n",
    "    l[label] = 1\n",
    "    return l\n",
    "\n",
    "\n",
    "dataset = vision.datasets.ImageFolder(\n",
    "    root=\"dataset\",\n",
    "    transform=vision.transforms.Compose([\n",
    "        v2.PILToTensor(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "    ]),\n",
    "    target_transform=target_transform)\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [TRAIN_SET_SIZE, 1 - TRAIN_SET_SIZE])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"The dataset has {len(dataset)} samples.\")\n",
    "print(f\"The train set has {len(train_set)} samples.\")\n",
    "print(f\"The test set has {len(test_set)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = models.Conv2Layers(n_classes=10, image_size=(64, 64))\n",
    "\n",
    "if has_cuda:\n",
    "    cls = cls.cuda()\n",
    "\n",
    "criterion = torch.nn.BCELoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(cls.parameters(), lr=1e-3)\n",
    "    \n",
    "loss_hist = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in tqdm.tqdm(train_loader):\n",
    "        if has_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        y = cls(inputs)\n",
    "        loss = criterion(y, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    loss_hist.append(total_loss)\n",
    "\n",
    "pyplot.plot(loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for inputs, labels in tqdm.tqdm(test_loader):\n",
    "    if has_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "    y = cls(inputs)\n",
    "    acc = torch.argmax(y, dim=1) == torch.argmax(labels, dim=1)\n",
    "    y = torch.argmax(y, dim=1).to(bool)\n",
    "    labels = torch.argmax(labels, dim=1).to(bool)\n",
    "    tp += ((y == True) & (labels == True)).sum()\n",
    "    tn += ((y == False) & (labels == False)).sum()\n",
    "    fp += ((y == True) & (labels == False)).sum()\n",
    "    fn += ((y == False) & (labels == True)).sum()\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(f\"Accuracy = {accuracy}\")\n",
    "print(f\"Precision = {precision}\")\n",
    "print(f\"Recall = {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": cls.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"outputs/cls_state.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Segmentation Model (U-Net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 27000 samples.\n",
      "The train set has 21601 samples.\n",
      "The test set has 5399 samples.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2000\n",
    "TRAIN_SET_SIZE = 0.8\n",
    "\n",
    "def target_transform_2(label: int) -> torch.Tensor:\n",
    "    \"\"\"Transform label into segmentation matrix.\"\"\"\n",
    "    l = torch.zeros((10, 64, 64))\n",
    "    l[label] = torch.ones((64, 64))\n",
    "    return l\n",
    "\n",
    "\n",
    "dataset = vision.datasets.ImageFolder(\n",
    "    root=\"dataset\",\n",
    "    transform=vision.transforms.Compose([\n",
    "        v2.PILToTensor(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "    ]),\n",
    "    target_transform=target_transform_2)\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [TRAIN_SET_SIZE, 1 - TRAIN_SET_SIZE])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"The dataset has {len(dataset)} samples.\")\n",
    "print(f\"The train set has {len(train_set)} samples.\")\n",
    "print(f\"The test set has {len(test_set)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filter(kernel_size, sigma=1, mean=0):\n",
    "    x, y = numpy.meshgrid(numpy.linspace(-1, 1, kernel_size), numpy.linspace(-1, 1, kernel_size))\n",
    "    dst = numpy.sqrt(x**2 + y**2)\n",
    "    gauss = numpy.exp(-((dst - mean)**2 / (2.0 * sigma**2)))\n",
    "    return gauss\n",
    "\n",
    "loss_filter = torch.Tensor(gaussian_filter(64))\n",
    "if has_cuda:\n",
    "    loss_filter = loss_filter.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_CHANNELS = [8, 12, 16, 20]\n",
    "\n",
    "unet = models.UNet(input_channels=3, n_classes=10, hidden_channels=HIDDEN_CHANNELS)\n",
    "if has_cuda:\n",
    "    unet = unet.cuda()\n",
    "\n",
    "criterion2 = torch.nn.BCELoss(reduction=\"none\")\n",
    "optimizer2 = torch.optim.Adam(unet.parameters(), lr=1e-3)\n",
    "\n",
    "loss_hist = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in tqdm.tqdm(train_loader):\n",
    "        if has_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        y = unet(inputs)\n",
    "        loss = criterion2(y, labels)\n",
    "        loss = (loss * loss_filter).mean()\n",
    "        optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "        total_loss += loss.item()\n",
    "    loss_hist.append(total_loss)\n",
    "\n",
    "pyplot.plot(loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for inputs, labels in tqdm.tqdm(test_loader):\n",
    "    if has_cuda:\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "    y = unet(inputs)\n",
    "    y = torch.argmax(y, dim=1).to(bool)\n",
    "    labels = torch.argmax(labels, dim=1).to(bool)\n",
    "    tp += ((y == True) & (labels == True)).sum()\n",
    "    tn += ((y == False) & (labels == False)).sum()\n",
    "    fp += ((y == True) & (labels == False)).sum()\n",
    "    fn += ((y == False) & (labels == True)).sum()\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(f\"Accuracy = {accuracy}\")\n",
    "print(f\"Precision = {precision}\")\n",
    "print(f\"Recall = {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": unet.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer2.state_dict(),\n",
    "    },\n",
    "    \"outputs/unet_state.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Segmentation Model (13 Bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_loader(path: str):\n",
    "    data = torch.load(path)\n",
    "    return data\n",
    "\n",
    "\n",
    "class PtDatasetFolder(vision.datasets.DatasetFolder):\n",
    "    def __init__(self, root: Union[str, Path], transform: Callable = None, target_transform: Callable = None):\n",
    "        super().__init__(\n",
    "            root=root,\n",
    "            loader=pt_loader,\n",
    "            extensions=[\".pt\"],\n",
    "            transform=transform,\n",
    "            target_transform=target_transform,\n",
    "            is_valid_file=None,\n",
    "            allow_empty=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 27000 samples.\n",
      "The train set has 21601 samples.\n",
      "The test set has 5399 samples.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2000\n",
    "TRAIN_SET_SIZE = 0.8\n",
    "\n",
    "def transform(data):\n",
    "    return data / 3500.0\n",
    "\n",
    "dataset = PtDatasetFolder(\n",
    "    root=\"dataset13_pt\",\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [TRAIN_SET_SIZE, 1 - TRAIN_SET_SIZE])\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"The dataset has {len(dataset)} samples.\")\n",
    "print(f\"The train set has {len(train_set)} samples.\")\n",
    "print(f\"The test set has {len(test_set)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filter(kernel_size, sigma=1, mean=0):\n",
    "    x, y = numpy.meshgrid(numpy.linspace(-1, 1, kernel_size), numpy.linspace(-1, 1, kernel_size))\n",
    "    dst = numpy.sqrt(x**2 + y**2)\n",
    "    gauss = numpy.exp(-((dst - mean)**2 / (2.0 * sigma**2)))\n",
    "    return gauss\n",
    "\n",
    "loss_filter = torch.Tensor(gaussian_filter(64))\n",
    "if has_cuda:\n",
    "    loss_filter = loss_filter.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = models.UNet(input_channels=13, n_classes=10, hidden_channels=[8, 12, 16, 20])\n",
    "if has_cuda:\n",
    "    unet = unet.cuda()\n",
    "\n",
    "criterion3 = torch.nn.BCELoss(reduction=\"none\")\n",
    "optimizer3 = torch.optim.Adam(unet.parameters(), lr=1e-3)\n",
    "\n",
    "loss_hist = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:29<00:00,  2.69s/it]\n",
      "100%|██████████| 11/11 [00:29<00:00,  2.68s/it]\n",
      "100%|██████████| 11/11 [00:29<00:00,  2.67s/it]\n",
      "100%|██████████| 11/11 [00:29<00:00,  2.65s/it]\n",
      "100%|██████████| 11/11 [00:28<00:00,  2.63s/it]\n",
      "100%|██████████| 11/11 [00:28<00:00,  2.62s/it]\n",
      "100%|██████████| 11/11 [00:28<00:00,  2.59s/it]\n",
      "100%|██████████| 11/11 [00:28<00:00,  2.60s/it]\n",
      "100%|██████████| 11/11 [00:28<00:00,  2.58s/it]\n",
      "100%|██████████| 11/11 [00:28<00:00,  2.61s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f5c151ed10>]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in tqdm.tqdm(train_loader):\n",
    "        labels_transformed = torch.zeros((labels.shape[0], 10, 64, 64), device=device)\n",
    "        for idx in range(labels.shape[0]):\n",
    "            labels_transformed[idx][labels[idx]] = torch.ones((64, 64), device=device)\n",
    "        \n",
    "        if has_cuda:\n",
    "            inputs = inputs.float().cuda()\n",
    "        \n",
    "        y = unet(inputs)\n",
    "        loss = criterion3(y, labels_transformed)\n",
    "        loss = (loss * loss_filter).mean()\n",
    "        optimizer3.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer3.step()\n",
    "        total_loss += loss.item()\n",
    "    loss_hist.append(total_loss)\n",
    "\n",
    "pyplot.plot(loss_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:07<00:00,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8873865604400635\n",
      "Precision = 0.8873865604400635\n",
      "Recall = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for inputs, labels in tqdm.tqdm(test_loader):\n",
    "    labels_transformed = torch.zeros((labels.shape[0], 10, 64, 64), device=device)\n",
    "    for idx in range(labels.shape[0]):\n",
    "        labels_transformed[idx][labels[idx]] = torch.ones((64, 64), device=device)\n",
    "    \n",
    "    if has_cuda:\n",
    "        inputs = inputs.float().cuda()\n",
    "    \n",
    "    y = unet(inputs)\n",
    "    y = torch.argmax(y, dim=1).to(bool)\n",
    "    labels = torch.argmax(labels_transformed, dim=1).to(bool)\n",
    "    tp += ((y == True) & (labels == True)).sum()\n",
    "    tn += ((y == False) & (labels == False)).sum()\n",
    "    fp += ((y == True) & (labels == False)).sum()\n",
    "    fn += ((y == False) & (labels == True)).sum()\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print(f\"Accuracy = {accuracy}\")\n",
    "print(f\"Precision = {precision}\")\n",
    "print(f\"Recall = {recall}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": unet.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer3.state_dict(),\n",
    "    },\n",
    "    \"outputs/unet13_state.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
